#!/usr/bin/env python
"""
Analyze pairs of closely-eluting spectra with similar precursor and fragments,
infer precursor and fragment error, and transform those values into values usable
as tolerance parameters by search engines.

If multiple files are specified, they will be processed together.
"""

import argparse
import logging
from datetime import datetime
from parammedic import errorcalc, ms2_io, mzml_io, mod_inference, enzyme_inference, util, binning
from parammedic import __version__
import gzip
from collections import OrderedDict

__author__ = "Damon May"
__copyright__ = "Copyright (c) 2016 Damon May"
__license__ = "Apache 2.0"

logger = logging.getLogger(__name__)
SPECTRUM_PROCESSED_REPORT_INTERVAL = 1000

# if we see fewer spectra than this, everything is questionable
MIN_SPECTRA_FOR_CONFIDENCE = 2000


def declare_gather_args():
    """
    Declare all arguments, parse them, and return the args dict.
    Does no validation beyond the implicit validation done by argparse.
    return: a dict mapping arg names to values
    """

    # declare args
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('infiles', type=argparse.FileType('r'), nargs='+',
                        help='input .mzML or .ms2 file(s). If multiple files are specified, \
                        they may be processed separately or together')

    parser.add_argument('--min-precursor-mz', type=float_greaterthanequalto0_type,
                        default=errorcalc.DEFAULT_MIN_MZ_FOR_BIN_PRECURSOR,
                        help='minimum precursor m/z value to use')
    parser.add_argument('--max-precursor-mz', type=float_greaterthanequalto0_type,
                        default=errorcalc.DEFAULT_MAX_MZ_FOR_BIN_PRECURSOR,
                        help='maximum precursor m/z value to use')
    parser.add_argument('--min-frag-mz', type=float_greaterthanequalto0_type,
                        default=errorcalc.DEFAULT_MIN_MZ_FOR_BIN_FRAGMENT,
                        help='minimum fragment m/z value to use')
    parser.add_argument('--max-frag-mz', type=float_greaterthanequalto0_type,
                        default=errorcalc.DEFAULT_MAX_MZ_FOR_BIN_FRAGMENT,
                        help='maximum fragment m/z value to use')
    parser.add_argument('--max-precursor-delta-ppm', type=float_greaterthanequalto0_type,
                        default=errorcalc.DEFAULT_MAX_PRECURSORDIST_PPM,
                        help='maximum ppm distance between precursor m/z values to consider two scans potentially \
                        generated by the same peptide')
    parser.add_argument('--charges', default=errorcalc.DEFAULT_CHARGES_STRING,
                        help='charge states to consider MS/MS spectra from (only most-abundant charge will be used)')
    parser.add_argument('--min-scan-frag-peaks', type=int_greaterthanequalto1_type,
                        default=util.MIN_SCAN_PEAKS,
                        help='Minimum fragment peaks an MS/MS scan must contain to be considered')
    parser.add_argument('--top-n-frag-peaks', type=int_greaterthanequalto1_type,
                        default=errorcalc.DEFAULT_TOPN_FRAGPEAKS,
                        help='number of most-intense fragment peaks to consider, per MS/MS spectrum')
    parser.add_argument('--min-common-frag-peaks', type=int_greaterthanequalto1_type,
                        default=errorcalc.DEFAULT_MIN_FRAGPEAKS_INCOMMON,
                        help='number of the most-intense peaks that two spectra must share in order to be \
                             potentially generated by the same peptide')
    parser.add_argument('--pair-top-n-frag-peaks', type=int_greaterthanequalto1_type,
                        default=errorcalc.DEFAULT_TOPN_FRAGPEAKS_FOR_ERROR_EST,
                        help='number of fragment peaks per spectrum pair to be used in fragment error estimation')
    parser.add_argument('--max-scan-separation', type=int_greaterthanequalto1_type,
                        default=errorcalc.DEFAULT_MAX_SCANS_BETWEEN_COMPARESCANS,
                        help='maximum number of scans two spectra can be separated by in order to be considered \
                        potentially generated by the same peptide')
    parser.add_argument('--min-peak-pairs', type=int_greaterthanequalto1_type,
                        default=errorcalc.DEFAULT_MIN_PEAKPAIRS_FOR_DISTRIBUTION_FIT,
                        help='minimum number of peak pairs (for precursor or fragment) that must be successfully \
                        paired in order to attempt to estimate error distribution')
    parser.add_argument('--process-together', action="store_true",
                        help='process multiple files as though they were a single file (default: process \
                         multiple files separately)')
    parser.add_argument('--noerrorcalc', action="store_true",
                        help='suppress precursor and fragment error calculation')
    parser.add_argument('--nomodinference', action="store_true",
                        help='suppress modification inference')
    parser.add_argument('--enzyme', action="store_true",
                        help='infer digestion enzyme')

    parser.add_argument('--debug', action="store_true", help='Enable debug logging')
    parser.add_argument('--version', action='version', version='%(prog)s {version}'.format(version=__version__))

    # these parameters are for development
    parser.add_argument('--maxspectra', type=int, help='Maximum spectra to process per file (or total, if process-together)')

    return parser.parse_args()


def int_greaterthanequalto1_type(x):
    """
    parameter type that is an integer >= 1
    :param x:
    :return:
    """
    x = int(x)
    if x < 1:
        raise argparse.ArgumentTypeError("Minimum value is 1")
    return x


def float_greaterthanequalto0_type(x):
    """
    parameter type that is a float >= 0
    :param x:
    :return:
    """
    x = float(x)
    if x < 0:
        raise argparse.ArgumentTypeError("Minimum value is 0.0")
    return x


def main():
    """
    Handle arguments, create all the objects that detect various things, process all the spectra, and infer parameters.
    :return:
    """
    args = declare_gather_args()
    if args.max_precursor_mz <= args.min_precursor_mz:
        quit("max-precursor-mz must be > min-precursor-mz")
    if args.max_frag_mz <= args.min_frag_mz:
        quit("max-frag-mz must be > min-frag-mz")

    # logging
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s %(levelname)s: %(message)s")
    if args.debug:
        logger.setLevel(logging.DEBUG)
        # add module-specific debuggers
        errorcalc.logger.setLevel(logging.DEBUG)
        mod_inference.logger.setLevel(logging.DEBUG)
        enzyme_inference.logger.setLevel(logging.DEBUG)
        ms2_io.logger.setLevel(logging.DEBUG)
        mzml_io.logger.setLevel(logging.DEBUG)
        util.logger.setLevel(logging.DEBUG)

    script_start_time = datetime.now()
    logger.debug("Start time: %s" % script_start_time)
    
    # each member of file_groups is a list of files that will be processed together.
    # The structure below allows for multiple groups of multiple files each, but in practice we're either
    # dealing with one group containing all the files, or a bunch of groups each containing one file.
    infile_groups = []
    if args.process_together:
        # process all files as a single group
        infile_groups.append(args.infiles)
    else:
        # process each file separately
        for infile in args.infiles:
            infile_groups.append([infile])
            
    charges_for_errorcalc = [int(x) for x in args.charges.split(",")]

    if args.process_together:
        logger.info("Processing all input files together.")
    elif len(infile_groups) > 1:
        logger.info("Processing input files separately.")

    # a list of dictionaries filled with the TSV output, one for each group of files
    output_name_value_dicts = []

    # loop over the groups of input files to process together
    for infile_group in infile_groups:
        # list of all run attribute detectors
        run_attribute_detectors = []
        # list of just the modification detectors (if any). Each modification detector produces a
        # detector_result, so we need to be able to iterate through just them.
        modification_detectors = []

        error_calculator = None
        if not args.noerrorcalc:
            # add the mass error calculator
            error_calculator = errorcalc.MultiChargeErrorCalculator(min_precursor_mz=args.min_precursor_mz,
                                                                    max_precursor_mz=args.max_precursor_mz,
                                                                    min_frag_mz=args.min_frag_mz,
                                                                    max_frag_mz=args.max_frag_mz,
                                                                    charges=charges_for_errorcalc,
                                                                    min_scan_frag_peaks=args.min_scan_frag_peaks,
                                                                    topn_frag_peaks=args.top_n_frag_peaks,
                                                                    min_common_frag_peaks=args.min_common_frag_peaks,
                                                                    pair_topn_frag_peaks=args.pair_top_n_frag_peaks,
                                                                    max_scan_separation=args.max_scan_separation,
                                                                    max_precursor_deltappm=args.max_precursor_delta_ppm,
                                                                    min_peakpairs=args.min_peak_pairs)
            run_attribute_detectors.append(error_calculator)

        # unless mod inference is suppressed, add each of the modification detectors
        if not args.nomodinference:
            silac_detector = mod_inference.SILACDetector()
            reporter_detector = mod_inference.ReporterIonProportionCalculator()
            phospho_detector = mod_inference.PhosphoLossProportionCalculator()
            modification_detectors.extend([silac_detector, reporter_detector, phospho_detector])
        run_attribute_detectors.extend(modification_detectors)

        # count spectra that go by
        n_spectra_processed = 0
        
        # map from activation type to count of scans with that type.
        activationtype_scancount_map = {}
        
        # process each file in the group
        for infile in infile_group:
            logger.info("Processing input file %s..." % infile.name)
            for spectrum in generate_spectra(infile, args.min_scan_frag_peaks):
                # report progress
                if n_spectra_processed % SPECTRUM_PROCESSED_REPORT_INTERVAL == 0:
                    logger.debug("  processed %d total spectra..." % n_spectra_processed)

                # update the activation type histogram
                if spectrum.activation_type:
                    if spectrum.activation_type not in activationtype_scancount_map:
                        activationtype_scancount_map[spectrum.activation_type] = 0
                    activationtype_scancount_map[spectrum.activation_type] += 1

                # bin the spectrum peaks
                binned_spectrum = binning.bin_spectrum(spectrum.mz_array, spectrum.intensity_array)
                # run each of the detectors on the binned peaks
                for run_attribute_detector in run_attribute_detectors:
                    run_attribute_detector.process_spectrum(spectrum, binned_spectrum)
                n_spectra_processed += 1

                # if stopping early, see if it's time to stop
                if (args.maxspectra is not None) and n_spectra_processed >= args.maxspectra:
                    logger.info("Stopping early after %d spectra." % n_spectra_processed)
                    break
            infile.close()
            if (args.maxspectra is not None) and n_spectra_processed >= args.maxspectra:
                break
        if n_spectra_processed == 0:
            logger.info("No spectra found! Quitting.")
            quit(1)
    
        logger.debug("Processed all spectra. Summarizing...")
        # build an OrderedDict of all the stuff to output in tsv format, so the columns are in a known order.
        output_name_value_dict = OrderedDict()
        output_name_value_dicts.append(output_name_value_dict)
        if len(infile_group) == 1:
            # just one file, use its name
            output_name_value_dict['file'] = infile_group[0].name
        else:
            # more than one file, so just call it 'multiple'
            output_name_value_dict['file'] = 'multiple'

        # accumulate all the search parameter recommendations
        search_parameter_recommendations = []
        if error_calculator:
            errorcalc_messages, precursor_sigma_ppm, frag_sigma_ppm, precursor_prediction_ppm, fragment_prediction_th = error_calculator.summarize()
            search_parameter_recommendations.extend(errorcalc_messages)
            # assemble error-calculation output. If a prediction failed the value will be None, so reformat
            output_name_value_dict['precursor_prediction_ppm'] = str(precursor_prediction_ppm) if precursor_prediction_ppm is not None else 'ERROR'
            output_name_value_dict['precursor_sigma_ppm'] = str(precursor_sigma_ppm) if precursor_sigma_ppm is not None else 'ERROR'
            output_name_value_dict['fragment_prediction_th'] = str(fragment_prediction_th) if fragment_prediction_th is not None else 'ERROR'
            output_name_value_dict['fragment_sigma_ppm'] = str(frag_sigma_ppm) if frag_sigma_ppm is not None else 'ERROR'

        # add search parameter suggestions for each detected modification
        detected_modifications = []
        for modification_detector in modification_detectors:
            detector_result = modification_detector.summarize()
            detected_modifications.extend(detector_result.search_modifications)
            for name in detector_result.name_value_pairs:
                output_name_value_dict[name] = detector_result.name_value_pairs[name]
        search_parameter_recommendations.extend(detected_modifications)

        # print out search parameter recommendations, if any.
        if search_parameter_recommendations:
            logger.info("\nSearch parameter recommendations:")
            for modification in search_parameter_recommendations:
                logger.info(modification)
        else:
            logger.info("No modifications detected requiring search parameter changes.")

        # conditionally analyze enzyme.
        # This functionality is highly speculative and, honestly, really shouldn't be here. Should be on a branch.
        if args.enzyme:
            # go back through the input files
            logger.info("Inferring enzyme...")
            n_spectra_processed = 0
            enzyme_detector = enzyme_inference.EnzymeDetector(detected_modifications)
            is_first_file = True
            for infile in infile_group:
                logger.info("Processing input file %s..." % infile.name)
                reopened_infile = open(infile.name)
                if not is_first_file:
                    for run_attribute_detector in run_attribute_detectors:
                        run_attribute_detector.next_file()
                logger.debug("Processing input file %s..." % infile.name)
                for spectrum in generate_spectra(reopened_infile):
                    binned_spectrum = binning.bin_spectrum(spectrum.mz_array, spectrum.intensity_array)
                    if n_spectra_processed % SPECTRUM_PROCESSED_REPORT_INTERVAL == 0:
                        logger.debug("  processed %d total spectra..." % n_spectra_processed)
                    enzyme_detector.process_spectrum(spectrum, binned_spectrum)
                    n_spectra_processed += 1
                    if (args.maxspectra is not None) and n_spectra_processed >= args.maxspectra:
                        logger.info("Stopping early after %d spectra." % n_spectra_processed)
                        break
                reopened_infile.close()
                is_first_file = False
            logger.info("Enzyme: %s" % enzyme_detector.summarize())

        # if we were able to capture any activation type information, add it to the output
        if activationtype_scancount_map:
            for activation_type in activationtype_scancount_map:
                output_name_value_dict['scancount_activation_%s' % activation_type] = \
                    str(activationtype_scancount_map[activation_type])
        else:
            logger.debug("No activation type information was available in input file(s)")

        # warn if we didn't see enough spectra.
        if n_spectra_processed < MIN_SPECTRA_FOR_CONFIDENCE:
            logger.info("WARNING: only %d spectra were analyzed. Confidence in all assessments is low."
                        % n_spectra_processed)

    # print tab-separated output.
    print("\t".join(output_name_value_dicts[0].keys()))
    for outdict in output_name_value_dicts:
        # if a value is "nan", change it to "ERROR".
        print("\t".join(format_val(outdict[x]) for x in outdict))


def format_val(x):
    """
    if x looks vaguely like a number, trim it to 5 decimal places. If not, leave it alone.
    Unless it's "nan", then change it to ERROR.
    This is brittle and silly, but it does the job for this specific use case.
    :param x:
    :return:
    """
    if "." in x:
        try:
            return "%.4f" % float(x)
        except ValueError:
            pass
    elif x == 'nan':
        return 'ERROR'
    return x


def generate_spectra(spectra_file, min_scan_frag_peaks):
    """
    a generator for MS2 and MS3 spectra from a .ms2 or .mzML file
    :param spectra_file:
    :return:
    """
    handle = spectra_file
    if spectra_file.name.endswith('.gz'):
        handle = gzip.open(spectra_file.name)
    if '.ms2' in spectra_file.name:
        io_module = ms2_io
    elif '.mzML' in spectra_file.name:
        io_module = mzml_io
    else:
        raise ValueError('generate_spectra, can\'t determine file type from name. Name=%s' % spectra_file.name)

    # send both scan levels 2 and 3 to each handler.
    for spectrum in io_module.read_scans(handle, ms_levels=(2,3)):
        # check peak count against threshold
        if len(spectrum.mz_array) >= min_scan_frag_peaks:
            yield spectrum

start_time = datetime.now()
logger.debug("Start time: %s" % start_time)
main()
logger.debug("End time: %s" % datetime.now())
logger.debug("Elapsed time: %s" % (datetime.now() - start_time))
